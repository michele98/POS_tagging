{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import urllib.request\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from collections import OrderedDict\n",
    "from typing import List, Callable, Dict\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
    "\n",
    "if not os.path.exists(dataset_folder):\n",
    "    os.makedirs(dataset_folder)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "\n",
    "dataset_path = os.path.join(dataset_folder, \"data.zip\")\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    urllib.request.urlretrieve(url, dataset_path)\n",
    "    print(\"Successful download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = (1, 101)\n",
    "val_range = (101, 151)\n",
    "test_range = (151, 200)\n",
    "\n",
    "split_sentences = True\n",
    "\n",
    "dataframe_rows = []\n",
    "with ZipFile(dataset_path, 'r') as myzip:\n",
    "    for i, filename in enumerate(myzip.namelist()[1:]):\n",
    "        print(\"Extracting\", filename, end='\\r')\n",
    "\n",
    "        with myzip.open(filename) as myfile:\n",
    "            file_id = int(filename.split('.')[0][-4:])\n",
    "\n",
    "            split = 'train'\n",
    "            if file_id in range(*val_range):\n",
    "                split = 'val'\n",
    "            elif file_id in range(*test_range):\n",
    "                split = 'test'\n",
    "\n",
    "            content_string = myfile.read().decode('utf-8')\n",
    "            if split_sentences:\n",
    "                sentences = content_string.split('\\n\\n')\n",
    "            else:\n",
    "                sentences = [content_string]\n",
    "\n",
    "            for sentence in sentences:\n",
    "                content = sentence.split('\\n')\n",
    "                content = [line.split('\\t') for line in content if len(line.split('\\t')) == 3]\n",
    "\n",
    "                words, tags, _ = zip(*content)\n",
    "\n",
    "                dataframe_rows.append({'file_id': file_id,\n",
    "                                       'text': ' '.join(words),\n",
    "                                       'tags': tags,\n",
    "                                       'split': split\n",
    "                                       })\n",
    "\n",
    "df = pd.DataFrame(dataframe_rows).sort_values('file_id').reset_index(drop=True)\n",
    "print(\"Dataframe created.\".ljust(50))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df[df['split'] == 'train']\n",
    "val_data = df[df['split'] == 'val']\n",
    "test_data = df[df['split'] == 'test']\n",
    "\n",
    "x_train = train_data['text'].values\n",
    "y_train = train_data['tags'].values\n",
    "\n",
    "x_val = val_data['text'].values\n",
    "y_val = val_data['tags'].values\n",
    "\n",
    "x_test = test_data['text'].values\n",
    "y_test = test_data['tags'].values\n",
    "\n",
    "print('Dataset splits statistics: ')\n",
    "print(f'Train data: {x_train.shape}')\n",
    "print(f'Validation data: {x_val.shape}')\n",
    "print(f'Test data: {x_test.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.kerasTokenizer import KerasTokenizer, convert_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_args = {\n",
    "    'oov_token': 'UNK',\n",
    "}\n",
    "embedding_dimension = 50\n",
    "tokenizer = KerasTokenizer(tokenizer_args=tokenizer_args,\n",
    "                           add_oov_terms=True,\n",
    "                           build_embedding_matrix=True,\n",
    "                           embedding_dimension=embedding_dimension,\n",
    "                           embedding_model_type=\"glove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.build_vocab(x_train)\n",
    "print(f'Tokenizer info: {tokenizer.get_info()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.build_vocab(x_val)\n",
    "print(f'Tokenizer info: {tokenizer.get_info()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.build_vocab(x_test)\n",
    "print(f'Tokenizer info: {tokenizer.get_info()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "x_train, max_seq_length = convert_text(texts=x_train, tokenizer=tokenizer, is_training=True)\n",
    "print(f\"Max token sequence: {max_seq_length}\")\n",
    "print(f'X train shape: {x_train.shape}')\n",
    "print(f'Y train shape: {y_train.shape}')\n",
    "\n",
    "# Val\n",
    "x_val = convert_text(texts=x_val, tokenizer=tokenizer, is_training=False, max_seq_length=max_seq_length)\n",
    "\n",
    "print(f'X val shape: {x_val.shape}')\n",
    "print(f'Y val shape: {y_val.shape}')\n",
    "\n",
    "# Test\n",
    "x_test = convert_text(texts=x_test, tokenizer=tokenizer, is_training=False, max_seq_length=max_seq_length)\n",
    "\n",
    "print(f'X test shape: {x_test.shape}')\n",
    "print(f'Y test shape: {y_test.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98b49351e707f46dcf9d0b031f5f36bb2db9d317180255edde60df3fbbef7ab4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
