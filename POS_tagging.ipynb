{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import urllib.request\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from collections import OrderedDict\n",
    "from typing import List, Callable, Dict\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
    "\n",
    "if not os.path.exists(dataset_folder):\n",
    "    os.makedirs(dataset_folder)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "\n",
    "dataset_path = os.path.join(dataset_folder, \"data.zip\")\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    urllib.request.urlretrieve(url, dataset_path)\n",
    "    print(\"Successful download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = (1, 101)\n",
    "val_range = (101, 151)\n",
    "test_range = (151, 200)\n",
    "\n",
    "split_sentences = True\n",
    "\n",
    "dataframe_rows = []\n",
    "with ZipFile(dataset_path, 'r') as myzip:\n",
    "    for i, filename in enumerate(myzip.namelist()[1:]):\n",
    "        print(\"Extracting\", filename, end='\\r')\n",
    "\n",
    "        with myzip.open(filename) as myfile:\n",
    "            file_id = int(filename.split('.')[0][-4:])\n",
    "\n",
    "            split = 'train'\n",
    "            if file_id in range(*val_range):\n",
    "                split = 'val'\n",
    "            elif file_id in range(*test_range):\n",
    "                split = 'test'\n",
    "\n",
    "            content_string = myfile.read().decode('utf-8')\n",
    "            if split_sentences:\n",
    "                sentences = content_string.split('\\n\\n')\n",
    "            else:\n",
    "                sentences = [content_string]\n",
    "\n",
    "            for sentence in sentences:\n",
    "                content = sentence.split('\\n')\n",
    "                content = [line.split('\\t') for line in content if len(line.split('\\t')) == 3]\n",
    "\n",
    "                words, tags, _ = zip(*content)\n",
    "\n",
    "                dataframe_rows.append({'file_id': file_id,\n",
    "                                       'text': ' '.join(words),\n",
    "                                       'tags': tags,\n",
    "                                       'split': split\n",
    "                                       })\n",
    "\n",
    "df = pd.DataFrame(dataframe_rows).sort_values('file_id').reset_index(drop=True)\n",
    "print(\"Dataframe created.\".ljust(50))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df[df['split'] == 'train']\n",
    "val_data = df[df['split'] == 'val']\n",
    "test_data = df[df['split'] == 'test']\n",
    "\n",
    "x_train = train_data['text'].values\n",
    "y_train = train_data['tags'].values\n",
    "\n",
    "x_val = val_data['text'].values\n",
    "y_val = val_data['tags'].values\n",
    "\n",
    "x_test = test_data['text'].values\n",
    "y_test = test_data['tags'].values\n",
    "\n",
    "print('Dataset splits statistics: ')\n",
    "print(f'Train data: {x_train.shape}')\n",
    "print(f'Validation data: {x_val.shape}')\n",
    "print(f'Test data: {x_test.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply GloVe embeddings and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oov_embedding(word, embedding_model, size):\n",
    "    \"\"\"For now just a random vector, can be changed to a more sophisticated method.\"\"\"\n",
    "    return np.random.uniform(low=-0.05, high=0.05, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.kerasTokenizer import load_embedding_model, check_OOV_terms\n",
    "\n",
    "print(\"Loading GloVe embedding.\")\n",
    "my_embedding_dimension = 50\n",
    "my_embedding_model = load_embedding_model('glove', my_embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GLOVE vocabulary (V1) size: \", len(my_embedding_model))\n",
    "\n",
    "x_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='[UNK]')\n",
    "\n",
    "print(\"Creating V2 using training set (V1 + OOV1)\")\n",
    "x_tokenizer.fit_on_texts(x_train)\n",
    "for word in tqdm(check_OOV_terms(my_embedding_model, x_tokenizer.word_index.keys())):\n",
    "    embedding_vector = get_oov_embedding(word=word, embedding_model=my_embedding_model, size=my_embedding_dimension)\n",
    "    my_embedding_model.__setitem__(word, embedding_vector)\n",
    "\n",
    "print(\"V2 size: \", len(my_embedding_model))\n",
    "\n",
    "print(\"Creating V3 using validation set (V2 + OOV2)\")\n",
    "x_tokenizer.fit_on_texts(x_val)\n",
    "for word in tqdm(check_OOV_terms(my_embedding_model, x_tokenizer.word_index.keys())):\n",
    "    embedding_vector = get_oov_embedding(word=word, embedding_model=my_embedding_model, size=my_embedding_dimension)\n",
    "    my_embedding_model.__setitem__(word, embedding_vector)\n",
    "\n",
    "print(\"V3 size: \", len(my_embedding_model))\n",
    "\n",
    "print(\"Creating V4 using validation set (V3 + OOV3)\")\n",
    "x_tokenizer.fit_on_texts(x_test)\n",
    "for word in tqdm(check_OOV_terms(my_embedding_model, x_tokenizer.word_index.keys())):\n",
    "    embedding_vector = get_oov_embedding(word=word, embedding_model=my_embedding_model, size=my_embedding_dimension)\n",
    "    my_embedding_model.__setitem__(word, embedding_vector)\n",
    "\n",
    "print(\"V4 size: \", len(my_embedding_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small variant, using x_tokenizer for indexing\n",
    "embedding_matrix = np.zeros((len(x_tokenizer.word_index), my_embedding_dimension))\n",
    "for i, word in enumerate(x_tokenizer.word_index.keys()):\n",
    "    embedding_matrix[i] = my_embedding_model.get_vector(word)\n",
    "\n",
    "\n",
    "# large variant, using embedding model for indexing\n",
    "# embedding_matrix = np.zeros((len(my_embedding_model), my_embedding_dimension))\n",
    "# for word, i in my_embedding_model.key_to_index.items():\n",
    "#     embedding_matrix[i] = my_embedding_model.get_vector(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_s = ' '.join([' '.join(y) for y in df['tags']])\n",
    "pd.DataFrame(tags_s.split())[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='[UNK]', filters='!\"%&()*+/;<=>?@[\\\\]^_{|}~\\t\\n', lower=False)\n",
    "y_tokenizer.fit_on_texts([' '.join(y) for y in df['tags']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_tokenizer.word_index.keys())\n",
    "print(len(y_tokenizer.word_index.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = tf.keras.preprocessing.sequence.pad_sequences(x_tokenizer.texts_to_sequences(df['text']), padding=\"post\")[0].size\n",
    "\n",
    "x_train_pad = tf.keras.preprocessing.sequence.pad_sequences(x_tokenizer.texts_to_sequences(x_train), maxlen=maxlen, padding=\"post\")\n",
    "x_val_pad = tf.keras.preprocessing.sequence.pad_sequences(x_tokenizer.texts_to_sequences(x_val), maxlen=maxlen, padding=\"post\")\n",
    "x_test_pad = tf.keras.preprocessing.sequence.pad_sequences(x_tokenizer.texts_to_sequences(x_test), maxlen=maxlen, padding=\"post\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98b49351e707f46dcf9d0b031f5f36bb2db9d317180255edde60df3fbbef7ab4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
