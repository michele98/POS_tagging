{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import urllib.request\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from collections import OrderedDict\n",
    "from typing import List, Callable, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
    "\n",
    "if not os.path.exists(dataset_folder):\n",
    "    os.makedirs(dataset_folder)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "\n",
    "dataset_path = os.path.join(dataset_folder, \"data.zip\")\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    urllib.request.urlretrieve(url, dataset_path)\n",
    "    print(\"Successful download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = (1, 101)\n",
    "val_range = (101, 151)\n",
    "test_range = (151, 200)\n",
    "\n",
    "split_sentences = True\n",
    "\n",
    "dataframe_rows = []\n",
    "with ZipFile(dataset_path, 'r') as myzip:\n",
    "    for i, filename in enumerate(myzip.namelist()[1:]):\n",
    "        print(\"Extracting\", filename, end='\\r')\n",
    "\n",
    "        with myzip.open(filename) as myfile:\n",
    "            file_id = int(filename.split('.')[0][-4:])\n",
    "\n",
    "            split = 'train'\n",
    "            if file_id in range(*val_range):\n",
    "                split = 'val'\n",
    "            elif file_id in range(*test_range):\n",
    "                split = 'test'\n",
    "\n",
    "            content_string = myfile.read().decode('utf-8')\n",
    "            if split_sentences:\n",
    "                sentences = content_string.split('\\n\\n')\n",
    "            else:\n",
    "                sentences = [content_string]\n",
    "\n",
    "            for sentence in sentences:\n",
    "                content = sentence.split('\\n')\n",
    "                content = [line.split('\\t') for line in content if len(line.split('\\t')) == 3]\n",
    "\n",
    "                words, tags, _ = zip(*content)\n",
    "\n",
    "                dataframe_rows.append({'file_id': file_id,\n",
    "                                       'text': ' '.join(words),\n",
    "                                       'tags': tags,\n",
    "                                       'split': split\n",
    "                                       })\n",
    "\n",
    "df = pd.DataFrame(dataframe_rows).sort_values('file_id').reset_index(drop=True)\n",
    "print(\"Dataframe created.\".ljust(50))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df[df['split'] == 'train']\n",
    "val_data = df[df['split'] == 'val']\n",
    "test_data = df[df['split'] == 'test']\n",
    "\n",
    "x_train = train_data['text'].values\n",
    "y_train = train_data['tags'].values\n",
    "\n",
    "x_val = val_data['text'].values\n",
    "y_val = val_data['tags'].values\n",
    "\n",
    "x_test = test_data['text'].values\n",
    "y_test = test_data['tags'].values\n",
    "\n",
    "print('Dataset splits statistics: ')\n",
    "print(f'Train data: {x_train.shape}')\n",
    "print(f'Validation data: {x_val.shape}')\n",
    "print(f'Test data: {x_test.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply GloVe embeddings and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oov_embedding(word, embedding_model, size):\n",
    "    \"\"\"For now just a random vector, can be changed to a more sophisticated method.\"\"\"\n",
    "    return np.random.uniform(low=-0.05, high=0.05, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.kerasTokenizer import load_embedding_model, check_OOV_terms\n",
    "\n",
    "print(\"Loading GloVe embedding.\")\n",
    "my_embedding_dimension = 50\n",
    "my_embedding_model = load_embedding_model('glove', my_embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GLOVE vocabulary (V1) size: \", len(my_embedding_model))\n",
    "\n",
    "x_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='[UNK]')\n",
    "\n",
    "print(\"Creating V2 using training set (V1 + OOV1)\")\n",
    "x_tokenizer.fit_on_texts(x_train)\n",
    "for word in tqdm(check_OOV_terms(my_embedding_model, x_tokenizer.word_index.keys())):\n",
    "    embedding_vector = get_oov_embedding(word=word, embedding_model=my_embedding_model, size=my_embedding_dimension)\n",
    "    my_embedding_model.__setitem__(word, embedding_vector)\n",
    "\n",
    "print(\"V2 size: \", len(my_embedding_model))\n",
    "\n",
    "print(\"Creating V3 using validation set (V2 + OOV2)\")\n",
    "x_tokenizer.fit_on_texts(x_val)\n",
    "for word in tqdm(check_OOV_terms(my_embedding_model, x_tokenizer.word_index.keys())):\n",
    "    embedding_vector = get_oov_embedding(word=word, embedding_model=my_embedding_model, size=my_embedding_dimension)\n",
    "    my_embedding_model.__setitem__(word, embedding_vector)\n",
    "\n",
    "print(\"V3 size: \", len(my_embedding_model))\n",
    "\n",
    "print(\"Creating V4 using validation set (V3 + OOV3)\")\n",
    "x_tokenizer.fit_on_texts(x_test)\n",
    "for word in tqdm(check_OOV_terms(my_embedding_model, x_tokenizer.word_index.keys())):\n",
    "    embedding_vector = get_oov_embedding(word=word, embedding_model=my_embedding_model, size=my_embedding_dimension)\n",
    "    my_embedding_model.__setitem__(word, embedding_vector)\n",
    "\n",
    "print(\"V4 size: \", len(my_embedding_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small variant, using x_tokenizer for indexing\n",
    "embedding_matrix = np.zeros((len(x_tokenizer.word_index), my_embedding_dimension))\n",
    "for i, word in enumerate(x_tokenizer.word_index.keys()):\n",
    "    embedding_matrix[i] = my_embedding_model.get_vector(word)\n",
    "\n",
    "\n",
    "# large variant, using embedding model for indexing\n",
    "# embedding_matrix = np.zeros((len(my_embedding_model), my_embedding_dimension))\n",
    "# for word, i in my_embedding_model.key_to_index.items():\n",
    "#     embedding_matrix[i] = my_embedding_model.get_vector(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_s = ' '.join([' '.join(y) for y in df['tags']])\n",
    "tag_types = pd.DataFrame(tags_s.split())[0].unique()\n",
    "tag_vocab = {t: i for i, t in enumerate(tag_types)}\n",
    "\n",
    "vocab_to_tag = {v: k for k, v in tag_vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = tf.keras.preprocessing.sequence.pad_sequences(x_tokenizer.texts_to_sequences(df['text']), padding=\"post\")[0].size\n",
    "\n",
    "x_train_pad = tf.keras.preprocessing.sequence.pad_sequences(x_tokenizer.texts_to_sequences(x_train), maxlen=maxlen, padding=\"post\")\n",
    "x_val_pad = tf.keras.preprocessing.sequence.pad_sequences(x_tokenizer.texts_to_sequences(x_val), maxlen=maxlen, padding=\"post\")\n",
    "x_test_pad = tf.keras.preprocessing.sequence.pad_sequences(x_tokenizer.texts_to_sequences(x_test), maxlen=maxlen, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_y(y):\n",
    "    return [[tag_vocab[tag] for tag in phrase] for phrase in y]\n",
    "\n",
    "y_train_tokenized = tokenize_y(y_train)\n",
    "y_val_tokenized = tokenize_y(y_val)\n",
    "y_test_tokenized = tokenize_y(y_test)\n",
    "\n",
    "y_train_pad = tf.keras.preprocessing.sequence.pad_sequences(y_train_tokenized, maxlen=maxlen, padding=\"post\")\n",
    "y_val_pad = tf.keras.preprocessing.sequence.pad_sequences(y_val_tokenized, maxlen=maxlen, padding=\"post\")\n",
    "y_test_pad = tf.keras.preprocessing.sequence.pad_sequences(y_test_tokenized, maxlen=maxlen, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizer_v2.adam import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from utils.training_utils import MyHistory\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_folder = \"weights\"\n",
    "\n",
    "checkpoint_partial = partial(ModelCheckpoint, monitor=\"val_loss\", mode=\"auto\")#, save_format=\"tf\")\n",
    "compile_args = dict(loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "input_shape = (maxlen, my_embedding_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import baselineLSTM\n",
    "\n",
    "optimizer = Adam(lr=1e-4)\n",
    "\n",
    "checkpoint_callback = checkpoint_partial(filepath = os.path.join(weights_folder, \"baseline\", \"checkpoint.hdf5\"))\n",
    "hist_callback = MyHistory(os.path.join(weights_folder, \"baseline\", \"history.npy\"))\n",
    "\n",
    "model = baselineLSTM(input_shape=input_shape, num_classes=len(tag_types))\n",
    "model.compile(**compile_args, optimizer=optimizer)\n",
    "\n",
    "history = model.fit(x=embedding_matrix[x_train_pad-1],\n",
    "                    y=y_train_pad,\n",
    "                    batch_size=32,\n",
    "                    epochs=2,\n",
    "                    validation_data=(embedding_matrix[x_val_pad-1],\n",
    "                                     y_val_pad),\n",
    "                    callbacks=[checkpoint_callback, hist_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GRUModel\n",
    "\n",
    "optimizer = Adam(lr=1e-4)\n",
    "\n",
    "checkpoint_callback = checkpoint_partial(filepath = os.path.join(weights_folder, \"gru\", \"checkpoint\"))\n",
    "hist_callback = MyHistory(os.path.join(weights_folder, \"gru\", \"history.npy\"))\n",
    "\n",
    "model = GRUModel(input_shape=input_shape, num_classes=len(tag_types))\n",
    "model.compile(**compile_args, optimizer=optimizer)\n",
    "\n",
    "history = model.fit(x=embedding_matrix[x_train_pad-1],\n",
    "                            y=y_train_pad,\n",
    "                            batch_size=32,\n",
    "                            epochs=2,\n",
    "                            validation_data=(embedding_matrix[x_val_pad-1],\n",
    "                                             y_val_pad),\n",
    "                            callbacks=[checkpoint_callback, hist_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import additionalLSTM\n",
    "\n",
    "optimizer = Adam(lr=1e-4)\n",
    "\n",
    "checkpoint_callback = checkpoint_partial(filepath = os.path.join(weights_folder, \"additional_lstm\", \"checkpoint\"))\n",
    "hist_callback = MyHistory(os.path.join(weights_folder, \"additional_lstm\", \"history.npy\"))\n",
    "\n",
    "model = additionalLSTM(input_shape=input_shape, num_classes=len(tag_types))\n",
    "model.compile(**compile_args, optimizer=optimizer)\n",
    "\n",
    "history = model.fit(x=embedding_matrix[x_train_pad-1],\n",
    "                            y=y_train_pad,\n",
    "                            batch_size=32,\n",
    "                            epochs=2,\n",
    "                            validation_data=(embedding_matrix[x_val_pad-1],\n",
    "                                             y_val_pad),\n",
    "                            callbacks=[checkpoint_callback, hist_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import additionalDense\n",
    "\n",
    "optimizer = Adam(lr=1e-4)\n",
    "\n",
    "checkpoint_callback = checkpoint_partial(filepath = os.path.join(weights_folder, \"additional_dense\", \"checkpoint\"))\n",
    "hist_callback = MyHistory(os.path.join(weights_folder, \"additional_dense\", \"history.npy\"))\n",
    "\n",
    "model = additionalDense(input_shape=input_shape, num_classes=len(tag_types))\n",
    "model.compile(**compile_args, optimizer=optimizer)\n",
    "\n",
    "history = model.fit(x=embedding_matrix[x_train_pad-1],\n",
    "                            y=y_train_pad,\n",
    "                            batch_size=32,\n",
    "                            epochs=2,\n",
    "                            validation_data=(embedding_matrix[x_val_pad-1],\n",
    "                                             y_val_pad),\n",
    "                            callbacks=[checkpoint_callback, hist_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "\n",
    "model = baselineLSTM(num_classes=45)\n",
    "\n",
    "model.load_weights(\"weights/baseline/checkpoint.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dist = model.predict(embedding_matrix[x_train_pad-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred_dist.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "results = {'ground truth': y_train[i], 'pred': tag_types[y_pred[i][:len(y_train[i])]]}\n",
    "pd.DataFrame(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "716c34c06793e5cf93c7b330bdfcb45c40eb89e5efd6d32b0d2191c531a6c3c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
