{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import urllib.request\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from collections import OrderedDict\n",
    "from typing import List, Callable, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "#os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.1.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
    "\n",
    "if not os.path.exists(dataset_folder):\n",
    "    os.makedirs(dataset_folder)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "\n",
    "dataset_path = os.path.join(dataset_folder, \"data.zip\")\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    urllib.request.urlretrieve(url, dataset_path)\n",
    "    print(\"Successful download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe created.                                \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tags</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>(Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
       "      <td>(NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>(Mr., Vinken, is, chairman, of, Elsevier, N.V....</td>\n",
       "      <td>(NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>(Rudolph, Agnew, ,, 55, years, old, and, forme...</td>\n",
       "      <td>(NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>(``, There, 's, no, question, that, some, of, ...</td>\n",
       "      <td>(``, EX, VBZ, DT, NN, IN, DT, IN, DT, NNS, CC,...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>(Workers, described, ``, clouds, of, blue, dus...</td>\n",
       "      <td>(NNS, VBD, ``, NNS, IN, JJ, NN, '', WDT, VBD, ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>198</td>\n",
       "      <td>(A, line-item, veto, is, a, procedure, that, w...</td>\n",
       "      <td>(DT, JJ, NN, VBZ, DT, NN, WDT, MD, VB, DT, NN,...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3910</th>\n",
       "      <td>198</td>\n",
       "      <td>(Sen., Kennedy, said, in, a, separate, stateme...</td>\n",
       "      <td>(NNP, NNP, VBD, IN, DT, JJ, NN, IN, PRP, VBZ, ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3911</th>\n",
       "      <td>199</td>\n",
       "      <td>(Trinity, Industries, Inc., said, it, reached,...</td>\n",
       "      <td>(NNP, NNPS, NNP, VBD, PRP, VBD, DT, JJ, NN, TO...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3912</th>\n",
       "      <td>199</td>\n",
       "      <td>(Terms, were, n't, disclosed, .)</td>\n",
       "      <td>(NNS, VBD, RB, VBN, .)</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3913</th>\n",
       "      <td>199</td>\n",
       "      <td>(Trinity, said, it, plans, to, begin, delivery...</td>\n",
       "      <td>(NNP, VBD, PRP, VBZ, TO, VB, NN, IN, DT, JJ, N...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3914 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      file_id                                               text  \\\n",
       "0           1  (Pierre, Vinken, ,, 61, years, old, ,, will, j...   \n",
       "1           1  (Mr., Vinken, is, chairman, of, Elsevier, N.V....   \n",
       "2           2  (Rudolph, Agnew, ,, 55, years, old, and, forme...   \n",
       "3           3  (``, There, 's, no, question, that, some, of, ...   \n",
       "4           3  (Workers, described, ``, clouds, of, blue, dus...   \n",
       "...       ...                                                ...   \n",
       "3909      198  (A, line-item, veto, is, a, procedure, that, w...   \n",
       "3910      198  (Sen., Kennedy, said, in, a, separate, stateme...   \n",
       "3911      199  (Trinity, Industries, Inc., said, it, reached,...   \n",
       "3912      199                   (Terms, were, n't, disclosed, .)   \n",
       "3913      199  (Trinity, said, it, plans, to, begin, delivery...   \n",
       "\n",
       "                                                   tags  split  \n",
       "0     (NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...  train  \n",
       "1     (NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...  train  \n",
       "2     (NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...  train  \n",
       "3     (``, EX, VBZ, DT, NN, IN, DT, IN, DT, NNS, CC,...  train  \n",
       "4     (NNS, VBD, ``, NNS, IN, JJ, NN, '', WDT, VBD, ...  train  \n",
       "...                                                 ...    ...  \n",
       "3909  (DT, JJ, NN, VBZ, DT, NN, WDT, MD, VB, DT, NN,...   test  \n",
       "3910  (NNP, NNP, VBD, IN, DT, JJ, NN, IN, PRP, VBZ, ...   test  \n",
       "3911  (NNP, NNPS, NNP, VBD, PRP, VBD, DT, JJ, NN, TO...   test  \n",
       "3912                             (NNS, VBD, RB, VBN, .)   test  \n",
       "3913  (NNP, VBD, PRP, VBZ, TO, VB, NN, IN, DT, JJ, N...   test  \n",
       "\n",
       "[3914 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_range = (1, 101)\n",
    "val_range = (101, 151)\n",
    "test_range = (151, 200)\n",
    "\n",
    "split_sentences = True\n",
    "\n",
    "dataframe_rows = []\n",
    "with ZipFile(dataset_path, 'r') as myzip:\n",
    "    for i, filename in enumerate(myzip.namelist()[1:]):\n",
    "        print(\"Extracting\", filename, end='\\r')\n",
    "\n",
    "        with myzip.open(filename) as myfile:\n",
    "            file_id = int(filename.split('.')[0][-4:])\n",
    "\n",
    "            split = 'train'\n",
    "            if file_id in range(*val_range):\n",
    "                split = 'val'\n",
    "            elif file_id in range(*test_range):\n",
    "                split = 'test'\n",
    "\n",
    "            content_string = myfile.read().decode('utf-8')\n",
    "            if split_sentences:\n",
    "                sentences = content_string.split('\\n\\n')\n",
    "            else:\n",
    "                sentences = [content_string]\n",
    "\n",
    "            for sentence in sentences:\n",
    "                content = sentence.split('\\n')\n",
    "                content = [line.split('\\t') for line in content if len(line.split('\\t')) == 3]\n",
    "\n",
    "                words, tags, _ = zip(*content)\n",
    "\n",
    "                dataframe_rows.append({'file_id': file_id,\n",
    "                                       'text': words,\n",
    "                                       'tags': tags,\n",
    "                                       'split': split\n",
    "                                       })\n",
    "\n",
    "df = pd.DataFrame(dataframe_rows).sort_values('file_id').reset_index(drop=True)\n",
    "print(\"Dataframe created.\".ljust(50))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tags</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[pierre, vinken, ,, 61, years, old, ,, will, j...</td>\n",
       "      <td>(NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[mr., vinken, is, chairman, of, elsevier, n.v....</td>\n",
       "      <td>(NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[rudolph, agnew, ,, 55, years, old, and, forme...</td>\n",
       "      <td>(NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[``, there, 's, no, question, that, some, of, ...</td>\n",
       "      <td>(``, EX, VBZ, DT, NN, IN, DT, IN, DT, NNS, CC,...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[workers, described, ``, clouds, of, blue, dus...</td>\n",
       "      <td>(NNS, VBD, ``, NNS, IN, JJ, NN, '', WDT, VBD, ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>198</td>\n",
       "      <td>[a, line-item, veto, is, a, procedure, that, w...</td>\n",
       "      <td>(DT, JJ, NN, VBZ, DT, NN, WDT, MD, VB, DT, NN,...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3910</th>\n",
       "      <td>198</td>\n",
       "      <td>[sen., kennedy, said, in, a, separate, stateme...</td>\n",
       "      <td>(NNP, NNP, VBD, IN, DT, JJ, NN, IN, PRP, VBZ, ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3911</th>\n",
       "      <td>199</td>\n",
       "      <td>[trinity, industries, inc., said, it, reached,...</td>\n",
       "      <td>(NNP, NNPS, NNP, VBD, PRP, VBD, DT, JJ, NN, TO...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3912</th>\n",
       "      <td>199</td>\n",
       "      <td>[terms, were, n't, disclosed, .]</td>\n",
       "      <td>(NNS, VBD, RB, VBN, .)</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3913</th>\n",
       "      <td>199</td>\n",
       "      <td>[trinity, said, it, plans, to, begin, delivery...</td>\n",
       "      <td>(NNP, VBD, PRP, VBZ, TO, VB, NN, IN, DT, JJ, N...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3914 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      file_id                                               text  \\\n",
       "0           1  [pierre, vinken, ,, 61, years, old, ,, will, j...   \n",
       "1           1  [mr., vinken, is, chairman, of, elsevier, n.v....   \n",
       "2           2  [rudolph, agnew, ,, 55, years, old, and, forme...   \n",
       "3           3  [``, there, 's, no, question, that, some, of, ...   \n",
       "4           3  [workers, described, ``, clouds, of, blue, dus...   \n",
       "...       ...                                                ...   \n",
       "3909      198  [a, line-item, veto, is, a, procedure, that, w...   \n",
       "3910      198  [sen., kennedy, said, in, a, separate, stateme...   \n",
       "3911      199  [trinity, industries, inc., said, it, reached,...   \n",
       "3912      199                   [terms, were, n't, disclosed, .]   \n",
       "3913      199  [trinity, said, it, plans, to, begin, delivery...   \n",
       "\n",
       "                                                   tags  split  \n",
       "0     (NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...  train  \n",
       "1     (NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...  train  \n",
       "2     (NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...  train  \n",
       "3     (``, EX, VBZ, DT, NN, IN, DT, IN, DT, NNS, CC,...  train  \n",
       "4     (NNS, VBD, ``, NNS, IN, JJ, NN, '', WDT, VBD, ...  train  \n",
       "...                                                 ...    ...  \n",
       "3909  (DT, JJ, NN, VBZ, DT, NN, WDT, MD, VB, DT, NN,...   test  \n",
       "3910  (NNP, NNP, VBD, IN, DT, JJ, NN, IN, PRP, VBZ, ...   test  \n",
       "3911  (NNP, NNPS, NNP, VBD, PRP, VBD, DT, JJ, NN, TO...   test  \n",
       "3912                             (NNS, VBD, RB, VBN, .)   test  \n",
       "3913  (NNP, VBD, PRP, VBZ, TO, VB, NN, IN, DT, JJ, N...   test  \n",
       "\n",
       "[3914 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(lambda l: [element.lower() for element in l])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits statistics: \n",
      "Train data: (1963,)\n",
      "Validation data: (1299,)\n",
      "Test data: (652,)\n"
     ]
    }
   ],
   "source": [
    "train_data = df[df['split'] == 'train']\n",
    "val_data = df[df['split'] == 'val']\n",
    "test_data = df[df['split'] == 'test']\n",
    "\n",
    "x_train = train_data['text'].values\n",
    "y_train = train_data['tags'].values\n",
    "\n",
    "x_val = val_data['text'].values\n",
    "y_val = val_data['tags'].values\n",
    "\n",
    "x_test = test_data['text'].values\n",
    "y_test = test_data['tags'].values\n",
    "\n",
    "print('Dataset splits statistics: ')\n",
    "print(f'Train data: {x_train.shape}')\n",
    "print(f'Validation data: {x_val.shape}')\n",
    "print(f'Test data: {x_test.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add OOV words to GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embedding.\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocessing import load_embedding_model, check_OOV_terms, get_OOV_embedding, add_OOV_embeddings\n",
    "\n",
    "print(\"Loading GloVe embedding.\")\n",
    "my_embedding_dimension = 50\n",
    "my_embedding_model = load_embedding_model('glove', my_embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOVE vocabulary size:  400000\n",
      "Add unknown token [UNK] and padding token \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1 size:  400002\n",
      "\n",
      "Creating V2 using training set (V1 + OOV1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 359/359 [00:13<00:00, 26.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V2 size:  400361\n",
      "\n",
      "Creating V3 using validation set (V2 + OOV2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [00:07<00:00, 24.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V3 size:  400550\n",
      "\n",
      "Creating V4 using validation set (V3 + OOV3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:07<00:00, 17.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V4 size:  400678\n"
     ]
    }
   ],
   "source": [
    "print(\"GLOVE vocabulary size: \", len(my_embedding_model))\n",
    "\n",
    "unknown_token = '[UNK]'\n",
    "padding_token = ''\n",
    "\n",
    "print(f\"Add unknown token {unknown_token} and padding token {padding_token}\")\n",
    "add_OOV_embeddings(my_embedding_model, [unknown_token, padding_token], my_embedding_dimension)\n",
    "print(\"V1 size: \", len(my_embedding_model))\n",
    "\n",
    "print(\"\\nCreating V2 using training set (V1 + OOV1)\")\n",
    "add_OOV_embeddings(my_embedding_model, x_train, my_embedding_dimension)\n",
    "print(\"V2 size: \", len(my_embedding_model))\n",
    "\n",
    "print(\"\\nCreating V3 using validation set (V2 + OOV2)\")\n",
    "add_OOV_embeddings(my_embedding_model, x_val, my_embedding_dimension)\n",
    "print(\"V3 size: \", len(my_embedding_model))\n",
    "\n",
    "print(\"\\nCreating V4 using validation set (V3 + OOV3)\")\n",
    "add_OOV_embeddings(my_embedding_model, x_test, my_embedding_dimension)\n",
    "print(\"V4 size: \", len(my_embedding_model))\n",
    "\n",
    "# build vocabulary for x\n",
    "dataset_vocabulary = np.unique([word for sentence in df['text'] for word in sentence])\n",
    "dataset_vocabulary = np.concatenate([[unknown_token], dataset_vocabulary])\n",
    "\n",
    "# build vocabulary for y\n",
    "tags_s = ' '.join([' '.join(y) for y in df['tags']])\n",
    "tag_vocabulary = pd.DataFrame(tags_s.split())[0].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The padding length is 44\n"
     ]
    }
   ],
   "source": [
    "padding_length = int(df['tags'].apply(lambda x: len(x)).quantile(0.95))\n",
    "print(\"The padding length is\", padding_length)\n",
    "\n",
    "dataset_dict = {k: i for i, k in enumerate(dataset_vocabulary)}\n",
    "\n",
    "def tokenize_x(x):\n",
    "    return [[dataset_dict[word] for word in phrase] for phrase in x]\n",
    "\n",
    "x_train_tokenized = tokenize_x(x_train)\n",
    "x_val_tokenized = tokenize_x(x_val)\n",
    "x_test_tokenized = tokenize_x(x_test)\n",
    "\n",
    "x_train_pad = tf.keras.preprocessing.sequence.pad_sequences(x_train_tokenized, maxlen=padding_length, padding=\"post\")\n",
    "x_val_pad = tf.keras.preprocessing.sequence.pad_sequences(x_val_tokenized, maxlen=padding_length, padding=\"post\")\n",
    "x_test_pad = tf.keras.preprocessing.sequence.pad_sequences(x_test_tokenized, maxlen=padding_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict = {k: i for i, k in enumerate(tag_vocabulary)}\n",
    "\n",
    "def tokenize_y(y):\n",
    "    return [[tag_dict[tag] for tag in phrase] for phrase in y]\n",
    "\n",
    "y_train_tokenized = tokenize_y(y_train)\n",
    "y_val_tokenized = tokenize_y(y_val)\n",
    "y_test_tokenized = tokenize_y(y_test)\n",
    "\n",
    "y_train_pad = tf.keras.preprocessing.sequence.pad_sequences(y_train_tokenized, maxlen=padding_length, padding=\"post\")\n",
    "y_val_pad = tf.keras.preprocessing.sequence.pad_sequences(y_val_tokenized, maxlen=padding_length, padding=\"post\")\n",
    "y_test_pad = tf.keras.preprocessing.sequence.pad_sequences(y_test_tokenized, maxlen=padding_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39madam\u001b[39;00m \u001b[39mimport\u001b[39;00m Adam\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m ModelCheckpoint\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m partial\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.optimizer'"
     ]
    }
   ],
   "source": [
    "from keras.optimizer.adam import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from functools import partial\n",
    "\n",
    "from utils.training_utils import MyHistory, plot_history\n",
    "from models import embedding_layer\n",
    "\n",
    "embedding_func = partial(embedding_layer,\n",
    "                         vocabulary=dataset_vocabulary,\n",
    "                         embedding_model=my_embedding_model,\n",
    "                         embedding_dimension=my_embedding_dimension)\n",
    "\n",
    "weights_folder = \"weights\"\n",
    "\n",
    "checkpoint_partial = partial(ModelCheckpoint, monitor=\"val_loss\", mode=\"auto\")#, save_format=\"tf\")\n",
    "compile_args = dict(loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "input_shape = (padding_length, my_embedding_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import baselineLSTM\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-4)\n",
    "\n",
    "checkpoint_path = os.path.join(weights_folder, \"baseline\", \"checkpoint.hdf5\")\n",
    "history_path = os.path.join(weights_folder, \"baseline\", \"history.npy\")\n",
    "\n",
    "checkpoint_callback = checkpoint_partial(filepath = checkpoint_path)\n",
    "hist_callback = MyHistory(history_path)\n",
    "\n",
    "model = baselineLSTM(num_classes=len(tag_vocabulary), input_shape=(padding_length,), embedding_func=embedding_func)\n",
    "model.compile(**compile_args, optimizer=optimizer)\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    model.load_weights(checkpoint_path)\n",
    "\n",
    "history = model.fit(x=x_train_pad,\n",
    "                    y=y_train_pad,\n",
    "                    batch_size=256,\n",
    "                    epochs=500,\n",
    "                    validation_data=(x_val_pad,\n",
    "                                     y_val_pad),\n",
    "                    callbacks=[checkpoint_callback, hist_callback])\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GRUModel\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-4)\n",
    "\n",
    "checkpoint_path = os.path.join(weights_folder, \"gru\", \"checkpoint.hdf5\")\n",
    "history_path = os.path.join(weights_folder, \"gru\", \"history.npy\")\n",
    "\n",
    "checkpoint_callback = checkpoint_partial(filepath = checkpoint_path)\n",
    "hist_callback = MyHistory(history_path)\n",
    "\n",
    "model = GRUModel(num_classes=len(tag_vocabulary), input_shape=(padding_length,), embedding_func=embedding_func)\n",
    "model.compile(**compile_args, optimizer=optimizer)\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    model.load_weights(checkpoint_path)\n",
    "\n",
    "history = model.fit(x=x_train_pad,\n",
    "                    y=y_train_pad,\n",
    "                    batch_size=256,\n",
    "                    epochs=500,\n",
    "                    validation_data=(x_val_pad,\n",
    "                                     y_val_pad),\n",
    "                    callbacks=[checkpoint_callback, hist_callback])\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import additionalLSTM\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-4)\n",
    "\n",
    "checkpoint_path = os.path.join(weights_folder, \"additionalLSTM\", \"checkpoint.hdf5\")\n",
    "history_path = os.path.join(weights_folder, \"additionalLSTM\", \"history.npy\")\n",
    "\n",
    "checkpoint_callback = checkpoint_partial(filepath = checkpoint_path)\n",
    "hist_callback = MyHistory(history_path)\n",
    "\n",
    "model = additionalLSTM(num_classes=len(tag_vocabulary), input_shape=(padding_length,), embedding_func=embedding_func)\n",
    "model.compile(**compile_args, optimizer=optimizer)\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    model.load_weights(checkpoint_path)\n",
    "\n",
    "history = model.fit(x=x_train_pad,\n",
    "                    y=y_train_pad,\n",
    "                    batch_size=256,\n",
    "                    epochs=500,\n",
    "                    validation_data=(x_val_pad,\n",
    "                                     y_val_pad),\n",
    "                    callbacks=[checkpoint_callback, hist_callback])\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import additionalDense\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-2)\n",
    "\n",
    "checkpoint_path = os.path.join(weights_folder, \"additionalDense\", \"checkpoint.hdf5\")\n",
    "history_path = os.path.join(weights_folder, \"additionalDense\", \"history.npy\")\n",
    "\n",
    "checkpoint_callback = checkpoint_partial(filepath = checkpoint_path)\n",
    "hist_callback = MyHistory(history_path)\n",
    "\n",
    "model = additionalDense(num_classes=len(tag_vocabulary), input_shape=(padding_length,), embedding_func=embedding_func)\n",
    "model.compile(**compile_args, optimizer=optimizer)\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    model.load_weights(checkpoint_path)\n",
    "\n",
    "history = model.fit(x=x_train_pad,\n",
    "                    y=y_train_pad,\n",
    "                    batch_size=256,\n",
    "                    epochs=500,\n",
    "                    validation_data=(x_val_pad,\n",
    "                                     y_val_pad),\n",
    "                    callbacks=[checkpoint_callback, hist_callback])\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdummy\u001b[39;00m \u001b[39mimport\u001b[39;00m DummyClassifier\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m classification_report\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m ConfusionMatrixDisplay\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_classifier = DummyClassifier(strategy=\"prior\")\n",
    "stratified_classifier = DummyClassifier(strategy=\"stratified\")\n",
    "\n",
    "majority_classifier.fit(x_train, y_train)\n",
    "stratified_classifier.fit(x_train, y_train)\n",
    "\n",
    "y_pred_train_maj = majority_classifier.predict(x_train)\n",
    "y_pred_test_maj = majority_classifier.predict(x_test)\n",
    "y_pred_train_st = stratified_classifier.predict(x_train)\n",
    "y_pred_test_st = stratified_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be filled when the models are trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dictionary with prediction info\n",
    "fill in this dictionary to get all the statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = [{\n",
    "    'model_label': 'maj',\n",
    "    'y_pred_train': majority_classifier.predict(x_train),\n",
    "    'y_pred_val': majority_classifier.predict(x_val),\n",
    "    'y_pred_test': majority_classifier.predict(x_test)\n",
    "    }, {\n",
    "    'model_label': 'st',\n",
    "    'y_pred_train': stratified_classifier.predict(x_train),\n",
    "    'y_pred_val': stratified_classifier.predict(x_val),\n",
    "    'y_pred_test': majority_classifier.predict(x_test)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze(y_true, y_pred, output_mode=0, model_label=None):\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    f1 = report['weighted avg']['f1-score']\n",
    "    if output_mode >= 1:\n",
    "        print(f\"{model_label}, weighted F1 macro: {report['weighted avg']['f1-score']:.2f}\")\n",
    "\n",
    "    if output_mode >= 2:\n",
    "        print(\"Confusion matrix\")\n",
    "        ConfusionMatrixDisplay.from_predictions(y_true, y_pred, cmap='Blues')\n",
    "        plt.show()\n",
    "    return f1\n",
    "\n",
    "output_mode=2\n",
    "for data in prediction_data:\n",
    "    data['f1_train'] = analyze(y_train, data['y_pred_train'], output_mode=output_mode, model_label=f\"{data['model_label']} train\")\n",
    "    data['f1_val'] = analyze(y_val, data['y_pred_val'], output_mode=output_mode, model_label=f\"{data['model_label']} validation\")\n",
    "    data['f1_test'] = analyze(y_test, data['y_pred_test'], output_mode=output_mode, model_label=f\"{data['model_label']} test\")\n",
    "\n",
    "f1_train = [data['f1_train'] for data in prediction_data]\n",
    "f1_val = [data['f1_val'] for data in prediction_data]\n",
    "f1_test = [data['f1_test'] for data in prediction_data]\n",
    "\n",
    "model_labels = [data['model_label'] for data in prediction_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar plot of F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h, dpi = 1280, 720, 100\n",
    "fig, ax = plt.subplots(figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "ax.bar(model_labels, height=f1_train, width=0.8)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23917085d79f400d74723ea31947e3bdd1120c2d5695aff2932efab284d060b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
