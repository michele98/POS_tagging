{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import urllib.request\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "import numpy as np\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
    "\n",
    "if not os.path.exists(dataset_folder):\n",
    "    os.makedirs(dataset_folder)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "\n",
    "dataset_path = os.path.join(dataset_folder, \"data.zip\")\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    urllib.request.urlretrieve(url, dataset_path)\n",
    "    print(\"Successful download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = (1, 101)\n",
    "val_range = (101, 151)\n",
    "test_range = (151, 200)\n",
    "\n",
    "split_sentences = True\n",
    "\n",
    "dataframe_rows = []\n",
    "with ZipFile(dataset_path, 'r') as myzip:\n",
    "    for i, filename in enumerate(myzip.namelist()[1:]):\n",
    "        print(\"Extracting\", filename, end='\\r')\n",
    "\n",
    "        with myzip.open(filename) as myfile:\n",
    "            file_id = int(filename.split('.')[0][-4:])\n",
    "\n",
    "            split = 'train'\n",
    "            if file_id in range(*val_range):\n",
    "                split = 'val'\n",
    "            elif file_id in range(*test_range):\n",
    "                split = 'test'\n",
    "\n",
    "            content_string = myfile.read().decode('utf-8')\n",
    "            if split_sentences:\n",
    "                sentences = content_string.split('\\n\\n')\n",
    "            else:\n",
    "                sentences = [content_string]\n",
    "\n",
    "            for sentence in sentences:\n",
    "                content = sentence.split('\\n')\n",
    "                content = [line.split('\\t') for line in content if len(line.split('\\t')) == 3]\n",
    "\n",
    "                words, tags, _ = zip(*content)\n",
    "\n",
    "                dataframe_rows.append({'file_id': file_id,\n",
    "                                       'text': ' '.join(words),\n",
    "                                       'tags': tags,\n",
    "                                       'split': split\n",
    "                                       })\n",
    "\n",
    "df = pd.DataFrame(dataframe_rows).sort_values('file_id').reset_index(drop=True)\n",
    "print(\"Dataframe created.\".ljust(50))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df[df['split'] == 'train']\n",
    "val_data = df[df['split'] == 'val']\n",
    "test_data = df[df['split'] == 'test']\n",
    "\n",
    "x_train = train_data['text'].values\n",
    "y_train = train_data['tags'].values\n",
    "\n",
    "x_val = val_data['text'].values\n",
    "y_val = val_data['tags'].values\n",
    "\n",
    "x_test = test_data['text'].values\n",
    "y_test = test_data['tags'].values\n",
    "\n",
    "print('Dataset splits statistics: ')\n",
    "print(f'Train data: {x_train.shape}')\n",
    "print(f'Validation data: {x_val.shape}')\n",
    "print(f'Test data: {x_test.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model(model_type: str,\n",
    "                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    \"\"\"\n",
    "    Loads a pre-trained word embedding model via gensim library.\n",
    "\n",
    "    :param model_type: name of the word embedding model to load.\n",
    "    :param embedding_dimension: size of the embedding space to consider\n",
    "\n",
    "    :return\n",
    "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
    "    \"\"\"\n",
    "    download_path = \"\"\n",
    "    if model_type.strip().lower() == 'word2vec':\n",
    "        download_path = \"word2vec-google-news-300\"\n",
    "\n",
    "    elif model_type.strip().lower() == 'glove':\n",
    "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    elif model_type.strip().lower() == 'fasttext':\n",
    "        download_path = \"fasttext-wiki-news-subwords-300\"\n",
    "    else:\n",
    "        raise AttributeError(\n",
    "            \"Unsupported embedding model type! Available ones: word2vec, glove, fasttext\")\n",
    "\n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Word2Vec: 300\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        print('FastText: 300')\n",
    "        raise e\n",
    "\n",
    "    return emb_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                    word_listing: List[str]):\n",
    "    \"\"\"\n",
    "    Checks differences between pre-trained embedding model vocabulary\n",
    "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_listing: dataset specific vocabulary (list)\n",
    "\n",
    "    :return\n",
    "        - list of OOV terms\n",
    "    \"\"\"\n",
    "    embedding_vocabulary = set(embedding_model.vocab.keys())\n",
    "    oov = set(word_listing).difference(embedding_vocabulary)\n",
    "    return list(oov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                           embedding_dimension: int,\n",
    "                           word_to_idx: Dict[str, int],\n",
    "                           vocab_size: int,\n",
    "                           oov_terms: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "    :param vocab_size: size of the vocabulary\n",
    "    :param oov_terms: list of OOV terms (list)\n",
    "\n",
    "    :return\n",
    "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.zeros(\n",
    "        (vocab_size, embedding_dimension), dtype=np.float32)\n",
    "    for word, idx in tqdm(word_to_idx.items()):\n",
    "        try:\n",
    "            embedding_vector = embedding_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(\n",
    "                low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasTokenizer(object):\n",
    "    \"\"\"\n",
    "    A simple high-level wrapper for the Keras tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, build_embedding_matrix=False, embedding_dimension=None,\n",
    "                 embedding_model_type=None, tokenizer_args=None):\n",
    "        if build_embedding_matrix:\n",
    "            assert embedding_model_type is not None\n",
    "            assert embedding_dimension is not None and type(embedding_dimension) == int\n",
    "        self.build_embedding_matrix = build_embedding_matrix\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.embedding_model_type = embedding_model_type\n",
    "        self.embedding_model = None\n",
    "        self.embedding_matrix = None\n",
    "        self.vocab = None\n",
    "        tokenizer_args = {} if tokenizer_args is None else tokenizer_args\n",
    "        assert isinstance(tokenizer_args, dict) or isinstance(tokenizer_args, OrderedDict)\n",
    "        self.tokenizer_args = tokenizer_args\n",
    "\n",
    "    def build_vocab(self, data, **kwargs):\n",
    "        print('Fitting tokenizer...')\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(**self.tokenizer_args)\n",
    "        self.tokenizer.fit_on_texts(data)\n",
    "        print('Fit completed!')\n",
    "        self.vocab = self.tokenizer.word_index\n",
    "        if self.build_embedding_matrix:\n",
    "            print('Loading embedding model! It may take a while...')\n",
    "            self.embedding_model = load_embedding_model(model_type=self.embedding_model_type,\n",
    "                                                        embedding_dimension=self.embedding_dimension)\n",
    "            \n",
    "            print('Checking OOV terms...')\n",
    "            self.oov_terms = check_OOV_terms(embedding_model=self.embedding_model,\n",
    "                                             word_listing=list(self.vocab.keys()))\n",
    "\n",
    "            print('Building the embedding matrix...')\n",
    "            self.embedding_matrix = build_embedding_matrix(embedding_model=self.embedding_model,\n",
    "                                                           word_to_idx=self.vocab,\n",
    "                                                           vocab_size=len(self.vocab) + 1,          \n",
    "                                                           embedding_dimension=self.embedding_dimension,\n",
    "                                                           oov_terms=self.oov_terms)\n",
    "            print('Done!')\n",
    "\n",
    "\n",
    "    def get_info(self):\n",
    "        return {\n",
    "            'build_embedding_matrix': self.build_embedding_matrix,\n",
    "            'embedding_dimension': self.embedding_dimension,\n",
    "            'embedding_model_type': self.embedding_model_type,\n",
    "            'embedding_matrix': self.embedding_matrix.shape if self.embedding_matrix is not None else self.embedding_matrix,\n",
    "            'embedding_model': self.embedding_model,\n",
    "            'vocab_size': len(self.vocab) + 1,\n",
    "        }\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return text\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        if type(tokens) == str:\n",
    "            return self.tokenizer.texts_to_sequences([tokens])[0]\n",
    "        else:\n",
    "            return self.tokenizer.texts_to_sequences(tokens)\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return self.tokenizer.sequences_to_texts(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: finish this\n",
    "\n",
    "# url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "\n",
    "# glove_path = os.path.join(os.getcwd(),\"Glove\")\n",
    "# glove_zip = os.path.join(os.getcwd(),\"Glove\", \"glove.6B.zip\")\n",
    "\n",
    "# if not os.path.exists(glove_path):\n",
    "#     os.makedirs(glove_path)\n",
    "\n",
    "# if not os.path.exists(glove_zip):\n",
    "#     urllib.request.urlretrieve(url, glove_zip)\n",
    "#     print(\"Successful download\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('DL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6ff426e1690047b832c49b85ace1a84a3b46a667c8dbfc5cdd12ae675de437c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
